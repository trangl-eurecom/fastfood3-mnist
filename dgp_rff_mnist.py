## Copyright 2016 Kurt Cutajar, Edwin V. Bonilla, Pietro Michiardi, Maurizio Filippone
##
## Licensed under the Apache License, Version 2.0 (the "License");
## you may not use this file except in compliance with the License.
## You may obtain a copy of the License at
##
##     http://www.apache.org/licenses/LICENSE-2.0
##
## Unless required by applicable law or agreed to in writing, software
## distributed under the License is distributed on an "AS IS" BASIS,
## WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
## See the License for the specific language governing permissions and
## limitations under the License.

import numpy as np
import tensorflow as tf

from tensorflow.contrib.learn.python.learn.datasets import base
from tensorflow.contrib.learn.python.learn.datasets.mnist import extract_images, extract_labels
from tensorflow.python.framework import dtypes


from dataset import DataSet
import utils
import likelihoods
from dgp_rff import DgpRff

# import baselines

import losses

from tensorflow.python.framework import ops
from tensorflow.python.ops import array_ops
"""
@ops.RegisterGradient("FastFood2")
def _fast_food2_grad(op, grad):
    x_tensor = op.inputs[0]
    shape = array_ops.shape(x_tensor)
    first_grad_x_tensor = array_ops.zeros(shape)
    
    other_tensor = op.inputs[1]
    shape = array_ops.shape(other_tensor)
    first_grad_other = array_ops.zeros(shape)
    
    return [first_grad_x_tensor, first_grad_other, first_grad_other, first_grad_other, first_grad_other]
"""
ops.NotDifferentiable("FastFood2")

def process_mnist(images, dtype = dtypes.float32, reshape=True):
    if reshape:
        assert images.shape[3] == 1
        images = images.reshape(images.shape[0],
                                images.shape[1] * images.shape[2])
    if dtype == dtypes.float32:
        # Convert from [0, 255] -> [0.0, 1.0].
        images = images.astype(np.float32)
        images = np.multiply(images, 1.0 / 255.0)

    return images

def get_data_info(images):
    rows, cols = images.shape
    std = np.zeros(cols)
    mean = np.zeros(cols)
    for col in range(cols):
        std[col] = np.std(images[:,col])
        mean[col] = np.mean(images[:,col])
    return mean, std

def standardize_data(images, means, stds):
    data = images.copy()
    rows, cols = data.shape
    for col in range(cols):
        if stds[col] == 0:
            data[:,col] = (data[:,col] - means[col])
        else:
            data[:,col] = (data[:,col] - means[col]) / stds[col]
    return data

def import_mnist():
    """
    This import mnist and saves the data as an object of our DataSet class
    :return:
    """
    SOURCE_URL = 'http://yann.lecun.com/exdb/mnist/'
    TRAIN_IMAGES = 'train-images-idx3-ubyte.gz'
    TRAIN_LABELS = 'train-labels-idx1-ubyte.gz'
    TEST_IMAGES = 't10k-images-idx3-ubyte.gz'
    TEST_LABELS = 't10k-labels-idx1-ubyte.gz'
    VALIDATION_SIZE = 0
    ONE_HOT = True
    TRAIN_DIR = 'MNIST_data'


    local_file = base.maybe_download(TRAIN_IMAGES, TRAIN_DIR,
                                     SOURCE_URL + TRAIN_IMAGES)
    train_images = extract_images(open(local_file))

    local_file = base.maybe_download(TRAIN_LABELS, TRAIN_DIR,
                                     SOURCE_URL + TRAIN_LABELS)
    train_labels = extract_labels(open(local_file), one_hot=ONE_HOT)

    local_file = base.maybe_download(TEST_IMAGES, TRAIN_DIR,
                                     SOURCE_URL + TEST_IMAGES)
    test_images = extract_images(open(local_file))

    local_file = base.maybe_download(TEST_LABELS, TRAIN_DIR,
                                     SOURCE_URL + TEST_LABELS)
    test_labels = extract_labels(open(local_file), one_hot=ONE_HOT)

    validation_images = train_images[:VALIDATION_SIZE]
    validation_labels = train_labels[:VALIDATION_SIZE]
    train_images = train_images[VALIDATION_SIZE:]
    train_labels = train_labels[VALIDATION_SIZE:]

    ## Process images
    train_images = process_mnist(train_images)
    validation_images = process_mnist(validation_images)
    test_images = process_mnist(test_images)

    ## Standardize data
    train_mean, train_std = get_data_info(train_images)
#    train_images = standardize_data(train_images, train_mean, train_std)
#    validation_images = standardize_data(validation_images, train_mean, train_std)
#    test_images = standardize_data(test_images, train_mean, train_std)

    data = DataSet(train_images, train_labels)
    test = DataSet(test_images, test_labels)
    val = DataSet(validation_images, validation_labels)

    return data, test, val

# This function will create a train set (a subset of data) such that the number of each class are the same (if possible)
def extract_balance_train_set(data, data_size, train_size, nb_class):
    all_train_set = data.next_batch(data_size)
    allX = all_train_set[0]
    allY = all_train_set[1]
    nbEachClass = int(train_size / nb_class)
    cur_nbEachClass = np.zeros([nb_class])
    trainX = []
    trainY = []
    ind = 0
    while (len(trainX) < train_size) and (ind < data_size):
        class_nb = np.multiply(allY[ind], np.arange(nb_class))
        class_nb = int(np.sum(class_nb))
        if (cur_nbEachClass[class_nb] < nbEachClass):
            trainX.append(allX[ind])
            trainY.append(allY[ind])
            cur_nbEachClass[class_nb] = cur_nbEachClass[class_nb] + 1
        ind = ind + 1
        
    if (ind >= data_size):
        trainX = allX[0:train_size]
        trainY = allY[0:train_size]
        
    trainX = np.array(trainX)
    trainY = np.array(trainY)
    return trainX, trainY
    
if __name__ == '__main__':
    
    FLAGS = utils.get_flags()

    ## Set random seed for tensorflow and numpy operations
    #tf.set_random_seed(FLAGS.seed)
    #np.random.seed(FLAGS.seed)

    data, test, _ = import_mnist()
    
    trainX, trainY = extract_balance_train_set(data, 55000, FLAGS.train_size, 10)
    testX_testY = test.next_batch(10000)
    testX = testX_testY[0]
    testY = testX_testY[1]
    
    
    ## Here we define a custom loss for dgp to show
    error_rate = losses.ZeroOneLoss(data.Dout)

    ## Likelihood
    like = likelihoods.Softmax()

    ## Optimizer
    optimizer = utils.get_optimizer(FLAGS.optimizer, FLAGS.learning_rate)

    ## Main dgp object
    #dgp = DgpRff(like, data.num_examples, data.X.shape[1], data.Y.shape[1], FLAGS.nl, FLAGS.n_rff, FLAGS.df, FLAGS.kernel_type, FLAGS.kernel_arccosine_degree, FLAGS.is_ard, FLAGS.feed_forward, FLAGS.q_Omega_fixed, FLAGS.theta_fixed, FLAGS.learn_Omega)
    dgp = DgpRff(like, len(trainX), len(trainX[0]), len(trainY[0]), FLAGS.nl, FLAGS.n_rff, FLAGS.df, FLAGS.kernel_type, FLAGS.kernel_arccosine_degree, FLAGS.is_ard, FLAGS.feed_forward, FLAGS.q_Omega_fixed, FLAGS.theta_fixed, FLAGS.learn_Omega)
    
    print("Learning phase")
    ## Learning
    dgp.learn(trainX, trainY, FLAGS.learning_rate, FLAGS.mc_train, FLAGS.batch_size, FLAGS.n_iterations, optimizer,
              FLAGS.display_step, testX, testY, FLAGS.mc_test, error_rate, FLAGS.duration, FLAGS.less_prints)